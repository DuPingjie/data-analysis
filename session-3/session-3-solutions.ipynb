{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3 : Supervised learning (2/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script `datasets.py` like you did in the last notebook to load the functions used to generate artificial datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the make_forge() function to generate a set of 500 points X and\n",
    "# their labels y for classification\n",
    "\n",
    "# make_forge() seems to always generate 4 less points than\n",
    "# the parameter we give. So to generate 500 points, we need\n",
    "# to pass the parameter 504.\n",
    "X, y = make_forge(504)\n",
    "print(\"X has\", len(X), \"points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this snippet prints the points on a 2d space\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset does not seem to be linearly separable. We are going to use a logistic regression model to create a classifier. Add the missing pieces of code to create a model and train it. Print its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# first thing before creating a model => Separate train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "print(\"Training set has\", len(X_train), \"points.\")\n",
    "print(\"Test     set has\", len(X_test), \"points.\")\n",
    "\n",
    "# then we can create a model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the decision boundary of this model like we did in the last notebook. Save the image (with code or the button in the GUI) for future model comparison. Find also the number of misclassified examples by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the decision boundary\n",
    "%run plots.py\n",
    "plot_2d_separator(model, X, y, fill=True, eps=0.5, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of misclassified points\n",
    "# First test if the predicted class is equal to the true\n",
    "# value. We get an array of booleans, each one indicates if\n",
    "# test_point[i] is misclassified.\n",
    "is_misclassified = model.predict(X_test) != y_test\n",
    "\n",
    "# then, we use the fact that True is evaluated as 1 and False\n",
    "# as 0. So if we sum all booleans, we get the number of times\n",
    "# there is a True value in the aray.\n",
    "n_misclassified = sum(is_misclassified)\n",
    "print(\"Number of misclassified points:\", n_misclassified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models for classification can also have a regularization. Read the source code of `LogisticRegression` to find the parameter used to control the regularization. Create other models (at least 4) with different values for this parameter. Print the accuracy of each of your model, the number of misclassified examples and the decision boundary. Save the image for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some LogisticRegression() models with regularization\n",
    "# Visualize their respective decision boundary\n",
    "\n",
    "# instead of saving each images, I plot the decision\n",
    "# boundaries in a 2x2 subplots figure (easier to see the\n",
    "# differences between boundaries)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,6))\n",
    "\n",
    "# model 1\n",
    "c = 0.001\n",
    "model_1 = LogisticRegression(C=c)\n",
    "model_1.fit(X_train, y_train)\n",
    "acc = model_1.score(X_test, y_test)\n",
    "print(\"Model 1\")\n",
    "print(\"  C =\", c)\n",
    "print(\"  Accuracy = \", acc)\n",
    "print(\"  Misclassified = \", \n",
    "      sum(model_1.predict(X_test) != y_test))\n",
    "plot_2d_separator(model_1, X, y, ax=ax[0][0],\n",
    "                  fill=True, eps=0.5, alpha=0.4)\n",
    "ax[0][0].set_title(\"C = {} / Acc = {}\".format(c, acc))\n",
    "\n",
    "# model 2\n",
    "c = 0.1\n",
    "model_2 = LogisticRegression(C=c)\n",
    "model_2.fit(X_train, y_train)\n",
    "acc = model_2.score(X_test, y_test)\n",
    "print(\"Model 2\")\n",
    "print(\"  C =\", c)\n",
    "print(\"  Accuracy = \", acc)\n",
    "print(\"  Misclassified = \", \n",
    "      sum(model_2.predict(X_test) != y_test))\n",
    "plot_2d_separator(model_2, X, y, ax=ax[0][1],\n",
    "                  fill=True, eps=0.5, alpha=0.4)\n",
    "ax[0][1].set_title(\"C = {} / Acc = {}\".format(c, acc))\n",
    "\n",
    "# model 3\n",
    "c = 5\n",
    "model_3 = LogisticRegression(C=c)\n",
    "model_3.fit(X_train, y_train)\n",
    "acc = model_3.score(X_test, y_test)\n",
    "print(\"Model 3\")\n",
    "print(\"  C =\", c)\n",
    "print(\"  Accuracy = \", acc)\n",
    "print(\"  Misclassified = \", \n",
    "      sum(model_3.predict(X_test) != y_test))\n",
    "plot_2d_separator(model_3, X, y, ax=ax[1][0],\n",
    "                  fill=True, eps=0.5, alpha=0.4)\n",
    "ax[1][0].set_title(\"C = {} / Acc = {}\".format(c, acc))\n",
    "\n",
    "# model 4\n",
    "c = 100\n",
    "model_4 = LogisticRegression(C=c)\n",
    "model_4.fit(X_train, y_train)\n",
    "acc = model_4.score(X_test, y_test)\n",
    "print(\"Model 4\")\n",
    "print(\"  C =\", c)\n",
    "print(\"  Accuracy = \", acc)\n",
    "print(\"  Misclassified = \", \n",
    "      sum(model_4.predict(X_test) != y_test))\n",
    "plot_2d_separator(model_4, X, y, ax=ax[1][1],\n",
    "                  fill=True, eps=0.5, alpha=0.4)\n",
    "ax[1][1].set_title(\"C = {} / Acc = {}\".format(c, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the images of decision boundaries you have saved. Can you see the influence of the regularization parameter ? Can this parameter help to prevent overfitting or underfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : the parameter C is the inverse of the regularization strength. This means that when `C=0.001`, the regularization\n",
    "is stronger than when `C=100`. With the decision boundaries, we can see that when the regularization is weak (`C=100`), the decision boudary is close to our data (the direction of the boundary is almost the same as the data, like a \\ line) but when it is strong (`C=0.001`), the boundary does not align with the data (like a / line).\n",
    "Like we said before, we need to find a `C` that balance the effect of the regularization. In our case, with `C=5`, we have a good accuracy and a boundary close to the horizontal line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the weight $w$ and $b$ learned by your models. Does the regularization parameter has an influence on the values of theses coefficients ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model 1\")\n",
    "print(\"  w =\", model_1.coef_)\n",
    "print(\"  b =\", model_1.intercept_)\n",
    "print(\"Model 2\")\n",
    "print(\"  w =\", model_2.coef_)\n",
    "print(\"  b =\", model_2.intercept_)\n",
    "print(\"Model 3\")\n",
    "print(\"  w =\", model_3.coef_)\n",
    "print(\"  b =\", model_3.intercept_)\n",
    "print(\"Model 4\")\n",
    "print(\"  w =\", model_4.coef_)\n",
    "print(\"  b =\", model_4.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : yes, `C` has an effect on the values of $w$ and $b$. We can see that, the stronger the regularization, the lower the values of $w$ and $b$. This is what we were expecting because the regularization tries to minimize the norm $\\left\\lVert W \\right\\rVert$. So if the model needs to optimize a lot the norm of $W$, its option is to decrease the values of $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see a toy example to visualize what are decision trees and how they create decision boundaries in the dataset. Run the following pieces of code, but first, run these two commands in a terminal in your machine (to install the software that can generate decision trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Python packages (if needed, add --proxy http://proxy-url:port)\n",
    "\n",
    "`sudo pip3 install graphviz`\n",
    "\n",
    "`sudo pip3 install pillow`\n",
    "\n",
    "###### Ubuntu package\n",
    "\n",
    "`sudo apt-get install graphviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run this cell\n",
    "%run plot_interactive_tree.py\n",
    "plot_tree_progressive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a decision tree on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the breast cancer dataset. Separate it into\n",
    "# a training and a test file\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "# create and train a decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy on training and test set. What can you \n",
    "# say about these scores ?\n",
    "training_accuracy = tree.score(X_train, y_train)\n",
    "test_accuracy = tree.score(X_test, y_test)\n",
    "print(\"Training accuracy:\", training_accuracy)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : we can see that the training accuracy is perfect (equal to 1) while the test accuracy is not. This means that our decision tree model is overfitting (good on training set, bad on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the documentation of DecisionTreeClassifier() and\n",
    "# train a model named tree that only has a depth of 4\n",
    "#?DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=4)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "training_accuracy = tree.score(X_train, y_train)\n",
    "test_accuracy = tree.score(X_test, y_test)\n",
    "print(\"Training accuracy:\", training_accuracy)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : now our training accuracy has decreased while our test accuracy has increased. This means that reducing the maximum depth of our decision tree model helps to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can save the DecisionTreeClassifier model as\n",
    "# a tree image.\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree, out_file=\"breast_tree.dot\", \n",
    "                class_names=[\"malignant\", \"benign\"],\n",
    "                feature_names=cancer.feature_names,\n",
    "                impurity=False,\n",
    "                filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ls, you can see that the file breast_tree.dot has\n",
    "# been created\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can visualize this .dot image with graphviz\n",
    "import graphviz\n",
    "with open(\"breast_tree.dot\") as f:\n",
    "    breast_tree = f.read()\n",
    "graphviz.Source(breast_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can visualize the importance of each features\n",
    "# learned by our model\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(tree.feature_importances_, 'o')\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "plt.ylim(0,1)\n",
    "fig.tight_layout() # so we can read all labels on x-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : \"Mean concave points\" and \"worst area\" are the most discriminative attributes in the decision tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
