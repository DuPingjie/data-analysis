{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4 : Supervised learning (3/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Run the script `datasets.py` like you did in the last notebook to load the functions used to generate artificial datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset of 300 points with make_forge()\n",
    "# and split it into a 270 points training set and \n",
    "# 30 points test set\n",
    "\n",
    "# make_forge() always create 4 less points than the number we\n",
    "# give. So to create 300 points, we need to pass the number\n",
    "# 304.\n",
    "X, y = make_forge(304)\n",
    "print(\"X has\", len(X), \"points.\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we can explicitly tell the number of points in the test set\n",
    "# with the test_size argument. All the other points will be\n",
    "# used as training points.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=30, random_state=0)\n",
    "print(\"X_train has\", len(X_train), \"points.\")\n",
    "print(\"X_test has\", len(X_test), \"points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training points on a 2d figure. Points with class\n",
    "# 1 should have the color 'salmon' and points with the class\n",
    "# 0 should have the color 'lightblue'\n",
    "\n",
    "# we need to create an array of colors representing the\n",
    "# color of each point. We use the function \n",
    "# where(condition, value1, value2) from the numpy library.\n",
    "# This function creates an array of only 2 different \n",
    "# values. If condition if true, value1 is used. Otherwise, \n",
    "# value2 is used.\n",
    "import numpy as np\n",
    "colors_train = np.where(y_train == 1, \"salmon\", \"lightblue\")\n",
    "\n",
    "# now we can print the points\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=colors_train, s=10)\n",
    "\n",
    "# print the test points on the same figure with color 'orange'\n",
    "# for class 1 and 'blue' for class 0\n",
    "\n",
    "# same thing for test points\n",
    "colors_test = np.where(y_test == 1, \"orange\", \"blue\")\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=colors_test, s=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SVM model for classification with SVC class.\n",
    "# Use a linear kernel. Train it and evaluate its accuracy\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"linear\")\n",
    "model.fit(X_train, y_train)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy of linear SVC =\", accuracy)\n",
    "\n",
    "# how many support vectors have been found for each class ? \n",
    "print(\"Class 0:\", model.n_support_[0], \"support vectors\")\n",
    "print(\"Class 1:\", model.n_support_[1], \"support vectors\")\n",
    "\n",
    "# Print them\n",
    "plt.scatter(\n",
    "    model.support_vectors_[:model.n_support_[1], 0],\n",
    "    model.support_vectors_[:model.n_support_[1], 1],\n",
    "    c=\"lightblue\", label=\"class 0\")\n",
    "plt.scatter(\n",
    "    model.support_vectors_[model.n_support_[1]:, 0],\n",
    "    model.support_vectors_[model.n_support_[1]:, 1],\n",
    "    c=\"salmon\", label=\"class 1\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run plots.py\n",
    "\n",
    "# visualize the decision boundary of your SVM. How many \n",
    "# points are misclassified ? Is your dataset linearly \n",
    "# separable ? Do you think it would be possible to improve \n",
    "# the accuracy of the model ? Explain why.\n",
    "\n",
    "plot_2d_separator(model, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "n_misclassified = sum(model.predict(X_test) != y_test)\n",
    "print(\"{}/{} misclassified points\".format(\n",
    "      n_misclassified, len(X_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : the dataset is not linearly separable because we cannot separate the red and blue points with a line. It might be possible to improve the accuracy since a slight modification (rotation of 5 degrees, anticlockwise) of the line could result in only one misclassified point (one blue point classified as a red point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train 5 other SVM with a modified value for the penalty and\n",
    "# plot the decision boundary for each one.\n",
    "# explain the effect of this parameter (what happend if we \n",
    "# increase/decrease it ?)\n",
    "# Do you think it can help to prevent underfitting or \n",
    "# overfitting ?\n",
    "\n",
    "model_1 = SVC(kernel=\"linear\", C=100)\n",
    "model_1.fit(X_train, y_train)\n",
    "print(\"Model 1 (C=100)\")\n",
    "print(\"  Train accuracy =\", model_1.score(X_train, y_train))\n",
    "print(\"  Test  accuracy =\", model_1.score(X_test, y_test))\n",
    "plot_2d_separator(model_1, X_test, y_test)\n",
    "plt.scatter(model_1.support_vectors_[:,0],\n",
    "            model_1.support_vectors_[:,1], c=\"lime\", s=2)\n",
    "plt.show()\n",
    "\n",
    "model_2 = SVC(kernel=\"linear\", C=10)\n",
    "model_2.fit(X_train, y_train)\n",
    "print(\"Model 2 (C=10)\")\n",
    "print(\"  Train accuracy =\", model_2.score(X_train, y_train))\n",
    "print(\"  Test  accuracy =\", model_2.score(X_test, y_test))\n",
    "plot_2d_separator(model_2, X_test, y_test)\n",
    "plt.scatter(model_2.support_vectors_[:,0],\n",
    "            model_2.support_vectors_[:,1], c=\"lime\", s=2)\n",
    "plt.show()\n",
    "\n",
    "model_3 = SVC(kernel=\"linear\", C=0.1)\n",
    "model_3.fit(X_train, y_train)\n",
    "print(\"Model 3 (C=0.1)\")\n",
    "print(\"  Train accuracy =\", model_3.score(X_train, y_train))\n",
    "print(\"  Test  accuracy =\", model_3.score(X_test, y_test))\n",
    "plot_2d_separator(model_3, X_test, y_test)\n",
    "plt.scatter(model_3.support_vectors_[:,0],\n",
    "            model_3.support_vectors_[:,1], c=\"lime\", s=2)\n",
    "plt.show()\n",
    "\n",
    "model_4 = SVC(kernel=\"linear\", C=0.01)\n",
    "model_4.fit(X_train, y_train)\n",
    "print(\"Model 4 (C=0.01)\")\n",
    "print(\"  Train accuracy =\", model_4.score(X_train, y_train))\n",
    "print(\"  Test  accuracy =\", model_4.score(X_test, y_test))\n",
    "plot_2d_separator(model_4, X_test, y_test)\n",
    "plt.scatter(model_4.support_vectors_[:,0],\n",
    "            model_4.support_vectors_[:,1], c=\"lime\", s=2)\n",
    "plt.show()\n",
    "\n",
    "model_5 = SVC(kernel=\"linear\", C=0.001)\n",
    "model_5.fit(X_train, y_train)\n",
    "print(\"Model 5 (C=0.001)\")\n",
    "print(\"  Train accuracy =\", model_5.score(X_train, y_train))\n",
    "print(\"  Test  accuracy =\", model_5.score(X_test, y_test))\n",
    "plot_2d_separator(model_5, X_test, y_test)\n",
    "plt.scatter(model_5.support_vectors_[:,0],\n",
    "            model_5.support_vectors_[:,1], c=\"lime\", s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : the parameter `C` represents the weight of each misclassified point. When `C` is high, the model tries to avoid as much as possible a misclassified point, even if it implies to have a smaller margin. We can see that when `C=100` or when `C=10`, the number of support vectors (in green) is small, and the distance between the black line and the furthest support vector is low (this distance is the margin).\n",
    "\n",
    "When `C` is low, the model tries to have a large margin, instead of trying to reduce the number of misclassified points (when `C` is low, the weight of an error is also low). We can see that with `C=0.001`, the number of support vectors is high (a lot of green points) and so is the margin. In this case, the test accuracy is perfect.\n",
    "\n",
    "The complexity of a SVM can be represented by the number of support vectors needed to draw the separating line. We can see that when `C` is low, there are a lot of support vectors, so our model is complex. And because we have a perfect test accuracy, we can suppose that our model is overfitting (too close to our dataset, not very able to generalize to unseen new data points). By increasing the value of `C`, we reduce the number of required support vectors, so is the complexity of our model. We are less likely to be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real case dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the breast cancer dataset and train a linear SVC model\n",
    "# on this dataset. You can create several models and adjust\n",
    "# the value of the penalty parameter to find the optimal one.\n",
    "# Can you get a better accuracy than the KNN model (it was \n",
    "# 0.923?)\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "model_breast = SVC(kernel=\"linear\", C=10, random_state=0)\n",
    "model_breast.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", model_breast.score(X_train, y_train))\n",
    "print(\"Test  accuracy:\", model_breast.score(X_test, y_test))\n",
    "\n",
    "# test accuracy is better compared to a KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function $f$ of a neuron can be linear or non linear. The most used activation functions are :\n",
    "* sigmoid\n",
    "* tanh\n",
    "* ReLu (rectified linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the example for sigmoid, plot the representation of\n",
    "# tanh and ReLu on the same graph. ReLu is defined as :\n",
    "# ReLu(x) = max(0, x)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "sigmoid_x = sigmoid(x)\n",
    "tanh_x    = np.tanh(x)\n",
    "ReLu_x    = np.maximum(0, x)\n",
    "\n",
    "figure = plt.figure()\n",
    "plt.plot(x, sigmoid_x, label=\"sigmoid\")\n",
    "plt.plot(x, tanh_x,    label=\"tanh\")\n",
    "plt.plot(x, ReLu_x,    label=\"ReLu\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a non linearly separable dataset that looks\n",
    "# like two moons\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using a linear SVM does not seem to be the best choice. Let's use neural networks to be able to classify this dataset. First, split the data into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a neural network with all default parameters.\n",
    "# Compute its accuracy and print its decision boundary\n",
    "# Do you think the neural network is good ? Explain why.\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "network = MLPClassifier()\n",
    "network.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", network.score(X_train, y_train))\n",
    "print(\"Test  accuracy:\", network.score(X_test, y_test))\n",
    "plot_2d_separator(network, X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : the accuracy of the neural network is not very good (0.84) and with the decision boundary, we can see that the network is not able to understand that the real decision boundary has a moon shape. Moreover, we have a warning message indicating that the network has not converged after 200 iterations (the default number of iterations). We need to tune our network to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there exist different algorithms to train a neural network.\n",
    "# the default one is called 'adam'. Use the documentation\n",
    "# to know what are the other kind of algorithm. Create \n",
    "# other networks with each type of algorithm. Which one is\n",
    "# the best ?\n",
    "\n",
    "# 3 types of solver are available in the MLPClassifier class:\n",
    "#   - adam\n",
    "#   - sgd\n",
    "#   - lbfgs\n",
    "\n",
    "nn_adam = MLPClassifier(solver=\"adam\")\n",
    "nn_adam.fit(X_train, y_train)\n",
    "print(\"Adam\")\n",
    "print(\"  Train accuracy:\", nn_adam.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_adam.score(X_test, y_test))\n",
    "\n",
    "nn_sgd = MLPClassifier(solver=\"sgd\")\n",
    "nn_sgd.fit(X_train, y_train)\n",
    "print(\"SGD\")\n",
    "print(\"  Train accuracy:\", nn_sgd.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_sgd.score(X_test, y_test))\n",
    "\n",
    "nn_lbfgs = MLPClassifier(solver=\"lbfgs\")\n",
    "nn_lbfgs.fit(X_train, y_train)\n",
    "print(\"LBFGS\")\n",
    "print(\"  Train accuracy:\", nn_lbfgs.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_lbfgs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : the 3 networks have almost the same test accuracy (between 0.8 and 0.84). The dataset is not large enough to make any conclusions on the performance increase/degradation when we swicth the training algorithm. But we can see that when using the LBFGS algorithm, we do not have any convergence warning message. As explained in the documentation, for small datasets LBFGS can converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, the neural network has only 1 hidden layer of\n",
    "# 100 neurons. Use the right parameter to create a network of\n",
    "# 2 hidden layers, each one having 10 neurons.\n",
    "\n",
    "nn_2layers = MLPClassifier(hidden_layer_sizes=(10, 10),\n",
    "                           max_iter=5000, random_state=0)\n",
    "nn_2layers.fit(X_train, y_train)\n",
    "print(\"2 hidden layers of 10 neurons\")\n",
    "print(\"  Train accuracy:\", nn_2layers.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_2layers.score(X_test, y_test))\n",
    "\n",
    "# Create other networks with different parameters to see if\n",
    "# many small layers is better than 1 big layer.\n",
    "\n",
    "nn_1 = MLPClassifier(hidden_layer_sizes=(1000),\n",
    "                     max_iter=5000, random_state=0)\n",
    "nn_1.fit(X_train, y_train)\n",
    "print(\"1 hidden layer of 1000 neurons\")\n",
    "print(\"  Train accuracy:\", nn_1.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_1.score(X_test, y_test))\n",
    "\n",
    "nn_2 = MLPClassifier(hidden_layer_sizes=(5, 5, 5, 5),\n",
    "                     max_iter=5000, random_state=0)\n",
    "nn_2.fit(X_train, y_train)\n",
    "print(\"4 hidden layers of 5 neurons\")\n",
    "print(\"  Train accuracy:\", nn_2.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_2.score(X_test, y_test))\n",
    "\n",
    "nn_3 = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                     max_iter=5000, random_state=0)\n",
    "nn_3.fit(X_train, y_train)\n",
    "print(\"2 hidden layers of 20 neurons\")\n",
    "print(\"  Train accuracy:\", nn_3.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : we can see that with 2 layers of 10 neurons, we have a better accuracy than with 1 layer of 100 neurons. In general, the only way to increase the accuracy of the model is to increase the number of neurons, because more neurons means that the model is more complex and can find non trivial boundaries. But having more layers is more efficient than having more neurons. If the activation function is non-linear, cascading this non-linearity can also create complex functions without requiring a lot of neurons.\n",
    "\n",
    "We usually start with a large amount of neurons, and see if we can get a good accuracy. With one layer of 1000 neurons, we have a test accuracy of 0.88 and a train accuracy of almost 0.99. This model is clearly overfitting. To prevent overfitting, we can reduce the number of neurons, or create more layers. With 4 layers of 5 neurons, we have a the same test and train accuracy, so we are still overfitting. Finally, with only 2 layers of 20 neurons (so a total of 40 neurons), we have a test accuracy of 0.92. The overfitting has been reduced and the performance increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also modify the activation function of the neurons \n",
    "# in your network. Create a network for each type of\n",
    "# activation. Which one is the best ?\n",
    "\n",
    "# identity\n",
    "nn_ident = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                         activation=\"identity\", \n",
    "                         max_iter=5000,\n",
    "                         random_state=0)\n",
    "nn_ident.fit(X_train, y_train)\n",
    "print(\"Identity activation\")\n",
    "print(\"  Train accuracy:\", nn_ident.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_ident.score(X_test, y_test))\n",
    "\n",
    "# sigmoid\n",
    "nn_sigm = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                        activation=\"logistic\", \n",
    "                        max_iter=5000,\n",
    "                        random_state=0)\n",
    "nn_sigm.fit(X_train, y_train)\n",
    "print(\"Sigmoid activation\")\n",
    "print(\"  Train accuracy:\", nn_sigm.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_sigm.score(X_test, y_test))\n",
    "\n",
    "# tanh\n",
    "nn_tanh = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                        activation=\"tanh\", \n",
    "                        max_iter=5000,\n",
    "                        random_state=0)\n",
    "nn_tanh.fit(X_train, y_train)\n",
    "print(\"Tanh activation\")\n",
    "print(\"  Train accuracy:\", nn_tanh.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_tanh.score(X_test, y_test))\n",
    "\n",
    "# relu\n",
    "nn_relu = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                        activation=\"relu\", \n",
    "                        max_iter=5000,\n",
    "                        random_state=0)\n",
    "nn_relu.fit(X_train, y_train)\n",
    "print(\"ReLu activation\")\n",
    "print(\"  Train accuracy:\", nn_relu.score(X_train, y_train))\n",
    "print(\"  Test  accuracy:\", nn_relu.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : in this case, ReLu activations are the best. This can be exlained with the graphic representations of the activation functions. During backpropagation, gradient is transfered layer from layer in reverse order, but the gradient is multiplied by the value of the derivative of the activation function everytime it is transfered. The derivative of a function at a certain point is equal to the slope of the function at this same point. With ReLu, the slope is very high compared to sigmoid or tanh. So the gradient have larger values, and the model can have a faster convergence, and so better results. In general, we need to test every activate function because the result depends on the dataset. \n",
    "\n",
    "**Tuning a neural network and testing (almost) every combinations of hyperparameters is time-consuming but is the key to have good performance with neural networks.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
