{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En fin de séance, sauvegarder le notebook en format .ipynb\n",
    "> (File -> Download as -> Notebook) et l'envoyer à\n",
    "> julien.tissier@univ-st-etienne.fr avec \"TD DA #2\" en \n",
    "> sujet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD2 : Supervised learning (1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the algorithms on real case datasets, we are going to experiment them on artificially generated datasets. We call these types of datasets **toy datasets**. Use the appropriate magic command to load the script `datasets.py` (it contains functions to generate toy datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use %load datasets.py, but this will just import all\n",
    "# the code of datasets.py into this cell. We can also use\n",
    "# %run datasets.py. This will run the script so all the functions\n",
    "# will be loaded and ready for future use\n",
    "%run datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors : Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to use is a set of points which have either the label `0` or `1`. Use the appropriate command to look at the source code of the function `make_forge()` and use it to create a set of points `X` and a set of labels `y`. How many points have been generated ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print source code of make_forge()\n",
    "??make_forge\n",
    "\n",
    "# by looking at the source code of make_forge(), we can\n",
    "# see that it returns two elements : X and y. So if we call\n",
    "# make forge, we need to assign it to two different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y. How many elements in X ?\n",
    "X, y = make_forge()\n",
    "print(\"First 5 rows of X\")\n",
    "print(X[:5]) # first 5 elements\n",
    "print(\"First 5 rows of y\")\n",
    "print(y[:5]) # first 5 elements\n",
    "print(\"There are\", len(X), \"elements in X.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the `matplotlib` library and use the right method to visualize a set of points on a 2D plan. Look at the documentation and use the approriate argument so that points labeled with `0` have a different color from the points with the label `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print the points X with matplotlib\n",
    "# to print a set of points, we need to use plt.scatter()\n",
    "# Let's first look at the documentation\n",
    "#plt.scatter?\n",
    "\n",
    "# we can read that scatter() takes 2 arguments: x and y\n",
    "# be careful, x and y refers to the x-axis and y-axis position\n",
    "# of our points. In our case, these information are the first and\n",
    "# second column of X. scatter() can also take another argument c\n",
    "# that specify the color of each point. We use the y variable\n",
    "# because it contains either 0 or 1 for each point\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the course, the first step is to separate our dataset into a training and a test part. Use the function `train_split_test()` library to create four variables :\n",
    "* points for training\n",
    "* labels for training\n",
    "* points for test\n",
    "* labels for test\n",
    "\n",
    "Use the parameter `random_state = 0` so the the experiments can be replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# by looking at the documentation of train_test_split,\n",
    "# we can see that it returns 4 variable, in that order :\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create a KNN model and specify the parameter `k`. Create a model with `k = 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# with the documentation, we can see that one argument of\n",
    "# KNeighborsClassifier is n_neighbors\n",
    "model = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on your training data (with the `.fit()` method) and evaluate its performance (with the `.score()` method) on the test data. How much accuracy do you get ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluation\n",
    "accuracy  = model.score(X_test, y_test)\n",
    "print(\"Accuracy =\", accuracy)\n",
    "print(\"# Misclassified points =\", (1 - accuracy) * len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an accuracy of 85.7%. Since we have 7 points in X_test, this means we have only 1 point that is misclassified (because 6 points correctly classified out of 7 gives 6/7 = 0.857)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see the boundary decision of our model (i.e. the line indicating where the points are labeled 0 or 1). Run the following piece of code to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run plots.py\n",
    "plot_2d_separator(model, X, y, fill=True, eps=0.5, alpha=0.4)\n",
    "plt.show()\n",
    "plt.title(\"Decision boundary when k=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create other models with a different value for `k` (use 1, 9 and 15). Train and evaluate each model. Which one is the best one ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1\n",
    "model_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "model_1.fit(X_train, y_train)\n",
    "accuracy_1 = model_1.score(X_test, y_test)\n",
    "print(\"Accuracy (k=1) =\", accuracy_1)\n",
    "\n",
    "# k = 9\n",
    "model_9 = KNeighborsClassifier(n_neighbors=9)\n",
    "model_9.fit(X_train, y_train)\n",
    "accuracy_9 = model_9.score(X_test, y_test)\n",
    "print(\"Accuracy (k=9) =\", accuracy_9)\n",
    "\n",
    "# k = 15\n",
    "model_15 = KNeighborsClassifier(n_neighbors=15)\n",
    "model_15.fit(X_train, y_train)\n",
    "accuracy_15 = model_15.score(X_test, y_test)\n",
    "print(\"Accuracy (k=15) =\", accuracy_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all models have the same accuracy (85.7%). This means that they all have misclassified only one point on test set (1 out of 7). With larger dataset, the difference would be more significative between models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the the decision boundary for each of these models. What can be said about the decision boundary when `k` is low ? When `k` is large ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision boundary for each models\n",
    "plot_2d_separator(model_1, X, y, fill=True, eps=0.5, alpha=0.4)\n",
    "plt.show()\n",
    "plt.title(\"Decision boundary when k=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_separator(model_9, X, y, fill=True, eps=0.5, alpha=0.4)\n",
    "plt.show()\n",
    "plt.title(\"Decision boundary when k=9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_separator(model_15, X, y, fill=True, eps=0.5, alpha=0.4)\n",
    "plt.show()\n",
    "plt.title(\"Decision boundary when k=15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When K is low , the boundary is very close to some points and sometimes try to englobe as many points as possible, even if that mean to create small circles. When K is higher, the boundary is more smooth and is more like a line.\n",
    "\n",
    "In general, when K is low the model is complex (every point will be well classified if enough neighbors are around) while when K is large, the model is simple (but some points are misclassified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn comes with some real case datasets. One of them is the Wisconsin breast cancer dataset. It contains information (measurements) of breast cancer tumors. Each tumor is either \"benign\" or \"malignant\" (so it is a binary classification problem). We are going to use KNN to predict if a tumor is \"benign\" or \"malignant\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "#print(cancer.DESCR) # uncomment for more information\n",
    "print(cancer.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 569 data points, each one has 30 attributes (called features). The data can be accessed with `cancer.data` and the labels with `cancer.target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.data.shape)\n",
    "print(cancer.data[0])\n",
    "print(cancer.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the points into a training and a test datasets with `random_state = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a KNN classifier with six neighbors and train it with the appropriate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_breast = KNeighborsClassifier(n_neighbors=6)\n",
    "model_breast.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of a classifier model is to be able to predict the label of points we have never seen yet. You can use the `.predict()` method of your classifier and feed it with one or more data points. The result will be the label(s) predicted by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_breast.predict([X_test[0]]) # replace model with the name of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the accuracy of your model on the entire test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_breast = model_breast.score(X_test, y_test)\n",
    "print(\"Accuracy on breast cancer =\", accuracy_breast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors : Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do regression with the KNN algorithm. Instead of assigning the most frequent label of the k nearest neighbors, we can average the value of the neighbors. Hence we predict a value instead of a class.\n",
    "\n",
    "Use the `make_wave()` function to create a toy dataset of `40` points for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_wave(40)\n",
    "print(\"X has\", len(X), \"points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X:\", X[:5])\n",
    "print(\"y:\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the points with the following piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xticks(X, \"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the dataset into a training part and a test part with `random_state = 0`. Then create several models for a KNN regression (at least 3 different models) with different values for the number of neighbors used. Train and evaluate them. What is your best accuracy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# create train + test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# create regression models, train and evaluate\n",
    "regmodel_2 = KNeighborsRegressor(n_neighbors=2)\n",
    "regmodel_2.fit(X_train, y_train)\n",
    "print(\"Accuracy (k=2)\", regmodel_2.score(X_test, y_test))\n",
    "\n",
    "regmodel_3 = KNeighborsRegressor(n_neighbors=3)\n",
    "regmodel_3.fit(X_train, y_train)\n",
    "print(\"Accuracy (k=3)\", regmodel_3.score(X_test, y_test))\n",
    "\n",
    "regmodel_4 = KNeighborsRegressor(n_neighbors=4)\n",
    "regmodel_4.fit(X_train, y_train)\n",
    "print(\"Accuracy (k=4)\", regmodel_4.score(X_test, y_test))\n",
    "\n",
    "regmodel_5 = KNeighborsRegressor(n_neighbors=5)\n",
    "regmodel_5.fit(X_train, y_train)\n",
    "print(\"Accuracy (k=5)\", regmodel_5.score(X_test, y_test))\n",
    "\n",
    "regmodel_7 = KNeighborsRegressor(n_neighbors=7)\n",
    "regmodel_7.fit(X_train, y_train)\n",
    "print(\"Accuracy (k=7)\", regmodel_7.score(X_test, y_test))\n",
    "\n",
    "print(\"Best accuracy achieved when k=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are mostly used to do regression (predicting a value given a set of features). You can use a linear model to do classification but we will focus on regression in this course. The predicted value $\\hat{y}$ can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y} = \\sum_{k=1}^n w_k \\times x_k + b\n",
    "\\end{equation*}\n",
    "\n",
    "where $x_k$ are the features of the data points, $w_k$ and $b$ are the parameters learned by the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Least Squares is the most classic linear method for regression. This model finds the $w$ and $b$ parameters that minimize the **mean squared error (MSE)** between predictions and the true value for the $m$ points in training dataset.\n",
    "\n",
    "\\begin{equation*}\n",
    "MSE = {1 \\over {m}} \\sum_{k=1}^m (\\hat{y}-y)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a toy dataset for regression with the function `make_wave()` composed of `80` data points. Then split this dataset into a training and a test dataset with `random_state = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_wave(n_samples=80)\n",
    "print(\"X has\", len(X), \"points.\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a linear model and train it on the right dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# create model and train it\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned $w$ are in the `coef_` attribute while the learned $b$ are in the `intercept_` attribute. Since our data only has one feature, we only have one $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learned w:\", model.coef_)\n",
    "print(\"Learned b:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as before, we can compute the estimated output with the `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model prediction =\", model.predict([X_test[0]]))\n",
    "print(\"Hand computed prediction =\", model.coef_[0] * X_test[0] + model.intercept_)\n",
    "print(\"Correct output =\", y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to predict the price of houses given some features. The data come from the housing market in Boston. We have 506 data points, and each one has 104 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_extended_boston()\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[:3]) # some house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data into a training set and a test set with\n",
    "# random_state = 0. Then train a linear model and predict the\n",
    "# price of the first house in the test set. Compare it with the \n",
    "# actual price of the house.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "model_house = LinearRegression()\n",
    "model_house.fit(X_train, y_train)\n",
    "print(\"Predicted price =\", model_house.predict([X_test[0]]))\n",
    "print(\"Real price =\", y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also compute the score of the model. Compare\n",
    "# the score obtained on the training data and the score\n",
    "# on the test data. \n",
    "# Do you thing we are underfitting or overfitting ? Explain why ?\n",
    "print(model_house.score(X_train, y_train))\n",
    "print(model_house.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are overfitting because our training accuracy is very good but our test accuracy is bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the linear model can overfit. This means that it will be good on the training set, but not on the test set. One way to control overfitting is to add a regularization to our model. We can add a constraint to the objective being minimized by the model.\n",
    "\n",
    "We will see a L2 normalization that minimizes the norm 2 of the weights $w$ of the model. The name of this new type of model is called **Ridge regression** and it minimizes :\n",
    "\n",
    "\\begin{equation*}\n",
    "MSE + Regularization = {1 \\over {m}} \\sum_{k=1}^m (\\hat{y}-y)^2 + \\lambda \\left\\lVert w \\right\\rVert ^2\n",
    "\\end{equation*}\n",
    "\n",
    "$\\lambda$ is a parameter to adjust the effect of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# create a model Ridge, train it on the same training\n",
    "# set made of the Housing market and evaluate its\n",
    "# training score and test score. Do you have any improvement ?\n",
    "# Is it better compared to a model with no regularization ?\n",
    "model_ridge = Ridge()\n",
    "model_ridge.fit(X_train, y_train)\n",
    "print(\"Training accuracy =\", model_ridge.score(X_train, y_train))\n",
    "print(\"Test accuracy =\", model_ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a better test accuracy and less difference between training and test accuracy. This means we have a more accurate model and have reduced the overfitting so this is a good improvement over the model which has no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different Ridge() models with different values for\n",
    "# the alpha parameter (read the documentation if necessary).\n",
    "# Then compute the training and test scores for each model.\n",
    "# Can you tell what is the influence of alpha on the scores ?\n",
    "model_ridge_01 = Ridge(alpha=0.1)\n",
    "model_ridge_01.fit(X_train, y_train)\n",
    "print(\"Training accuracy (alpha=0.1) =\", model_ridge_01.score(X_train, y_train))\n",
    "print(\"Test accuracy     (alpha=0.1) =\", model_ridge_01.score(X_test, y_test))\n",
    "\n",
    "model_ridge_02 = Ridge(alpha=0.2)\n",
    "model_ridge_02.fit(X_train, y_train)\n",
    "print(\"Training accuracy (alpha=0.2) =\", model_ridge_02.score(X_train, y_train))\n",
    "print(\"Test accuracy     (alpha=0.2) =\", model_ridge_02.score(X_test, y_test))\n",
    "\n",
    "model_ridge_05 = Ridge(alpha=0.5)\n",
    "model_ridge_05.fit(X_train, y_train)\n",
    "print(\"Training accuracy (alpha=0.5) =\", model_ridge_05.score(X_train, y_train))\n",
    "print(\"Test accuracy     (alpha=0.5) =\", model_ridge_05.score(X_test, y_test))\n",
    "\n",
    "model_ridge_2 = Ridge(alpha=2)\n",
    "model_ridge_2.fit(X_train, y_train)\n",
    "print(\"Training accuracy (alpha=2)   =\", model_ridge_2.score(X_train, y_train))\n",
    "print(\"Test accuracy     (alpha=2)   =\", model_ridge_2.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When alpha (lambda in the formula) is increased, the regularization is more effective hence the gap between training and test accuracy is reduced. We are preventing overfitting as we increase alpha.\n",
    "\n",
    "But when alpha is too big, the model tries to minimize the norm of W more than the MSE. The performance starts to become bad. The main idea is to find the best alpha that maximizes the performance of our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
