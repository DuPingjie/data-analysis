{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5 : Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that PCA can be used as a pre-processing step before using supervised learning algorithms in order to improve accuracy or training speed. But sometimes, a simple pre-processing step like normalizing the data can bring a huge improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before seing any improvements, we need to have a **baseline** (to know if we improve or deteriorate accuracy). We are going to train a classification SVM (with non-linear kernel). Do the following operations :\n",
    "* load the bread cancer dataset\n",
    "* separate it into a training and a test set\n",
    "* create a SVC model, with C=100\n",
    "* train your model, print its accuracy\n",
    "\n",
    "I know that you already did it in the last session, but try to see if you can do it again on your own, without any code snippet provided or looking at the correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "model = SVC(C=100)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Accuracy =\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using built-in normalizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our data looks like before normalization. For each feature, print its minimum and maximum value across all examples in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "minima = np.min(cancer.data, axis=0)\n",
    "maxima = np.max(cancer.data, axis=0)\n",
    "\n",
    "for i in range(len(cancer.feature_names)):\n",
    "    print(\"{:<25}: MIN = {:6.2f} / MAX = {:7.2f}\".format(\n",
    "          cancer.feature_names[i], minima[i], maxima[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use a normalizer, to make sure that each feature has a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# look at the documentation of MinMaxScaler to find examples\n",
    "# of use case. Then create a new variable X_train_scaled that\n",
    "# is the rescaled X_train. Print again the maximum and\n",
    "# minimum values in X_train_scaled for each features. What is\n",
    "# the difference ?\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(cancer.data)\n",
    "\n",
    "# make sure the MinMaxScaler found the same minimum and\n",
    "# maximum values as us\n",
    "same_min = np.isclose(scaler.data_min_, minima)\n",
    "print(\"Scaler found same minima ?\", np.all(same_min))\n",
    "same_max = np.isclose(scaler.data_max_, maxima)\n",
    "print(\"Scaler found same maxima ?\", np.all(same_max))\n",
    "\n",
    "# then we can rescale our training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# print the new min and max values\n",
    "print(\"\\nMinima of X_train_scaled\")\n",
    "print(np.min(X_train_scaled, axis=0))\n",
    "print(\"\\nMaxima of X_train_scaled\")\n",
    "print(np.max(X_train_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : we can see that the scaler did its job : every features are now between 0 (miminum) and 1 (maximum). But we can see that some features have a minimum not equal to 0, or a maximum not equal to 1. This is normal. The scaler was trained on the whole dataset (cancer.data). So the min and max values found by the scaler (and used for the transformation) are not always from X_train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same scaler (without modifying it) to also rescale the `X_test` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train again a SVC model, but this time train it on the scaled data. Do you see any improvements ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_minmax = SVC(C=100)\n",
    "svc_minmax.fit(X_train_scaled, y_train)\n",
    "print(\"Accuracy =\", svc_minmax.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : we have a huge improvement (from 0.629 to 0.965)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more different types of scaler are implemented in scikit-learn :\n",
    "* MaxAbsScaler\n",
    "* RobustScaler\n",
    "* StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use each type of scaler and train a SVC model for each one.\n",
    "# Which one is the best ?\n",
    "\n",
    "# MaxAbsScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "scaler_maxabs = MaxAbsScaler()\n",
    "scaler_maxabs.fit(cancer.data)\n",
    "\n",
    "X_train_scaled_maxabs = scaler_maxabs.transform(X_train)\n",
    "X_test_scaled_maxabs  = scaler_maxabs.transform(X_test)\n",
    "\n",
    "svc_maxabs = SVC(C=100)\n",
    "svc_maxabs.fit(X_train_scaled_maxabs, y_train)\n",
    "print(\"[MaxAbsScaler] accuracy  =\", svc_maxabs.score(\n",
    "    X_test_scaled_maxabs, y_test))\n",
    "\n",
    "# RobustScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler_robust = RobustScaler()\n",
    "scaler_robust.fit(cancer.data)\n",
    "\n",
    "X_train_scaled_robust = scaler_robust.transform(X_train)\n",
    "X_test_scaled_robust  = scaler_robust.transform(X_test)\n",
    "\n",
    "svc_robust = SVC(C=100)\n",
    "svc_robust.fit(X_train_scaled_robust, y_train)\n",
    "print(\"[RobustScaler] accuracy  =\", svc_robust.score(\n",
    "    X_test_scaled_robust, y_test))\n",
    "\n",
    "# StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_standard.fit(cancer.data)\n",
    "\n",
    "X_train_scaled_standard = scaler_standard.transform(X_train)\n",
    "X_test_scaled_standard  = scaler_standard.transform(X_test)\n",
    "\n",
    "svc_standard = SVC(C=100)\n",
    "svc_standard.fit(X_train_scaled_standard, y_train)\n",
    "print(\"[StandardScaler] accuracy  =\", svc_standard.score(\n",
    "    X_test_scaled_standard, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : the best scaler for the breast dataset seems to be the MaxAbsScaler. The worst one seems to be the RobustScaler. Maybe we do not have a lot of outlier in our dataset, so the robust scaler is not very efficient. A deeper analysis of the dataset would be necessary to answer that question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look also at the features of your scaled dataset to see\n",
    "# differences between the different scalers (do some of\n",
    "# them keep negative values ? does it change the dimension\n",
    "# mean/median?)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"MinMaxScaler\")\n",
    "print(\"  Means:\", X_train_scaled.mean(axis=0)[:5])\n",
    "print(\"  Mins:\", X_train_scaled.min(axis=0)[:5])\n",
    "print(\"  Maxs:\", X_train_scaled.max(axis=0)[:5])\n",
    "\n",
    "print(\"MaxAbsScaler\")\n",
    "print(\"  Means:\", X_train_scaled_maxabs.mean(axis=0)[:5])\n",
    "print(\"  Mins:\", X_train_scaled_maxabs.min(axis=0)[:5])\n",
    "print(\"  Maxs:\", X_train_scaled_maxabs.max(axis=0)[:5])\n",
    "\n",
    "print(\"RobustScaler\")\n",
    "print(\"  Means:\", X_train_scaled_robust.mean(axis=0)[:5])\n",
    "print(\"  Mins:\", X_train_scaled_robust.min(axis=0)[:5])\n",
    "print(\"  Maxs:\", X_train_scaled_robust.max(axis=0)[:5])\n",
    "\n",
    "print(\"StandarScaler\")\n",
    "print(\"  Means:\", X_train_scaled_standard.mean(axis=0)[:5])\n",
    "print(\"  Mins:\", X_train_scaled_standard.min(axis=0)[:5])\n",
    "print(\"  Maxs:\", X_train_scaled_standard.max(axis=0)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : the MinMaxScaler rescale the data between 0 and 1, so every values are positive. The MaxAbsScaler rescale the data so that the maximum absolute value is 1. Every value are positive because the original dataset have positive values. But the minimum is not 0, so the mean value for each feature is higher compared to the MinMaxScaler.\n",
    "\n",
    "The RobustScaler and StandardScaler have other kind of transformation. We can see that the scaled data have some negative values, even if none of them were negative in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for 2D visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going use the PCA algorithm on the breast cancer dataset. This dataset has 30 features, so we can not visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you didn't do it before, scale the breast cancer\n",
    "# dataset with a StandardScaler(). We do not need to have a\n",
    "# training and test set, because we want to visualize the \n",
    "# entire dataset, so you can apply the scaler on the entire\n",
    "# dataset.\n",
    "\n",
    "# scaler has been trained in previous part\n",
    "X_scaled = scaler_standard.transform(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can use PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# use the documentation of PCA to create a model that will\n",
    "# only keep 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# then call the .fit() method of PCA on your scaled data\n",
    "pca.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now transform the dataset with 30 features into\n",
    "# a dataset with only 2 features\n",
    "X_pca = pca.transform(X_scaled)\n",
    "print(\"Shape before PCA:\", X_scaled.shape)\n",
    "print(\"Shape after PCA:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot the new dataset (the one with only 2 features)\n",
    "# on a 2D plan. Use a different color for the two classes.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=cancer.target, s=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum, interesting... It looks like our\n",
    "dataset is almost linearly separable. This means that a\n",
    "linear model (like SVM with linear kernel or logistic regression) could do quite well on this dataset. Let's see if that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by splitting the X_pca dataset into a training\n",
    "# and a test set with random_state=7 (because we are going \n",
    "# to train on this new dataset composed of only 2 features)\n",
    "\n",
    "X_pca_train, X_pca_test, y_train, y_test = train_test_split(\n",
    "    X_pca, cancer.target, random_state=7)\n",
    "\n",
    "model_pca = SVC(kernel=\"linear\", C=100)\n",
    "model_pca.fit(X_pca_train, y_train)\n",
    "print(\"Accuracy =\", model_pca.score(X_pca_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : pre-processing the data with PCA before training a linear SVM (a simpler model than a non-linear SVM) brings accuracy from 0.629 to 0.972."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for 3D visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can reduce any dataset with $n$ features into a dataset with 2 or 3 features. And matplotlib can draw functions and points in 3D, so we can project our data into a 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same process as before so you get a new\n",
    "# dataset for the breast cancer dataset that only contains \n",
    "# 3 features.\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "pca_3d.fit(X_scaled)\n",
    "X_pca_3d = pca_3d.transform(X_scaled)\n",
    "print(\"Shape before PCA:\", X_scaled.shape)\n",
    "print(\"Shape after PCA:\", X_pca_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize it in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# use the function below, but replace the value x, y and z\n",
    "# with the appropriate one from your dataset that contains\n",
    "# 3 features. Separate the 2 different classes with 2 \n",
    "# different colors.\n",
    "ax.scatter(X_pca_3d[:,0], X_pca_3d[:,1], X_pca_3d[:,2],\n",
    "           c=cancer.target, s=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are going to implement from scratch the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are going to use are inside `data-clustering.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first 10 rows of this file with the bash\n",
    "# command head\n",
    "!head -n 10 data_clustering.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we have 2 columns, named V1 and V2. Let's load it into 2 ndarrays : x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lines = open(\"data_clustering.csv\").read().split()[1:]\n",
    "x = np.array([line.split(',')[0] for line in lines], \n",
    "             dtype=np.float32)\n",
    "y = np.array([line.split(',')[1] for line in lines], \n",
    "             dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot the points (x, y) with matplotlib. Modify the \n",
    "# value of the argument s so that points are not too big.\n",
    "plt.scatter(x, y, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many clusters do you think there are ?\n",
    "\n",
    "# there are 3 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm requires a distance (so we can compute which centroid is closer for each point). Implement a function `distance` that takes 2 arguments (two vectors as ndarray) and return the distance between them. Hint : the distance between two vectors can be computed with \n",
    "\\begin{equation}\n",
    "d(u, v) = \\sqrt{\\sum_{i=0}^k (u_i - v_i)^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(v1, v2, ax=0):\n",
    "    return np.linalg.norm(v1 - v2, axis=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: 1-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the complete algorithm, let's start with only one step. You need the following things before starting :\n",
    "* define a constant variable K\n",
    "* declare an empty array `clusters` that has the same size as x. We will put in each cell $i$ the cluster assigned to $x_i$\n",
    "* create an array `centroids` where you will store the centroids\n",
    "\n",
    "Then implement only one step from the algorithm described in the course (i.e. one iteration of the **while** loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# regroup the x and y data into a single matrix (composed\n",
    "# of n rows, and 2 columns. .T is the transpose operation.\n",
    "points = np.array([x, y]).T\n",
    "\n",
    "# initialization. randint(0, len(x), K) will choose K random\n",
    "# indexes between 0 and the size of x (the second argument\n",
    "# is exclusive, so randint() cannot return an index equal to\n",
    "# len(x)). Then we directly select the K points with the []\n",
    "# notation.\n",
    "K = 3\n",
    "clusters  = np.zeros(len(points))\n",
    "centroids = points[np.random.randint(0, len(x), K)]\n",
    "\n",
    "# print the selected centroids\n",
    "print(\"Initial centroids:\")\n",
    "for i in range(K):\n",
    "    print(\"  => \", centroids[i])\n",
    "\n",
    "# define a function that achieve one step of the k-means\n",
    "# algorithm\n",
    "def one_step(points, centroids, clusters):\n",
    "    \"\"\"Return the new centroids computed after the step.\"\"\"\n",
    "    \n",
    "    # declare an array of 0 that has the same shape as\n",
    "    # centroids\n",
    "    new_centroids = np.zeros_like(centroids)\n",
    "    \n",
    "    # assign cluster for each point\n",
    "    for idx, point in enumerate(points):\n",
    "        # compute the distance to each centroid. Find the\n",
    "        # closest one\n",
    "        min_distance = distance(point, centroids[0])\n",
    "        index_closest_centroid = 0\n",
    "        \n",
    "        # start at 1 because we already processed centroid[0]\n",
    "        for k in range(1, len(centroids)):\n",
    "            d = distance(point, centroids[k])\n",
    "            if (d < min_distance):\n",
    "                min_distance = d\n",
    "                index_closest_centroid = k\n",
    "        \n",
    "        # then we can know the cluster of point (it is the \n",
    "        # same as the index of the closest centroid)\n",
    "        clusters[idx] = index_closest_centroid\n",
    "        \n",
    "    # compute new centroids. For each cluster, compute the\n",
    "    # average point. This average point is the new centroid.\n",
    "    for k in range(len(centroids)):\n",
    "        coordinates  = np.zeros(2)\n",
    "        cluster_size = 0\n",
    "        for idx, point in enumerate(points):\n",
    "            if clusters[idx] == k:\n",
    "                coordinates  += point\n",
    "                cluster_size += 1\n",
    "        \n",
    "        new_centroids[k] = coordinates / cluster_size\n",
    "        \n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running one iteration, you can plot on the same graph :\n",
    "* all points\n",
    "* the first centroids chosen at random\n",
    "* the new updated centroids\n",
    "\n",
    "Do you see the beginning of an improvement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print point after initialization\n",
    "plt.scatter(x, y, c=\"lightblue\", s=2, label=\"data\")\n",
    "plt.scatter(centroids[:,0], centroids[:,1],\n",
    "            marker=\"x\", c=\"red\", label=\"centroid (step 0)\")\n",
    "\n",
    "# do one step, and print new centroids\n",
    "new_centroids = one_step(points, centroids, clusters)\n",
    "plt.scatter(new_centroids[:,0], new_centroids[:,1],\n",
    "            marker=\"x\", c=\"m\", label=\"centroid (step 1)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : we can see that one centroid is moving to the bottom-right corner cluster. This means the algorithm is starting to move the centroids to their respective cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm : mutiple steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm will repeat steps like the one you created a certain amount of time before convergence. We can consider the algorithm has converged when it does not update the values of centroids any longer (i.e. the distance between old and updated centroids is 0 for each centroid). Implement the full algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(points, K):\n",
    "    # initialization\n",
    "    clusters  = np.zeros(len(points))\n",
    "    centroids = points[np.random.randint(0, len(points), K)]\n",
    "\n",
    "    # one first step\n",
    "    new_centroids = one_step(points, centroids, clusters)\n",
    "    \n",
    "    e = 0.1\n",
    "    while distance(new_centroids, centroids, 1).sum() > e:\n",
    "        # copy the new_centroids into centroids\n",
    "        centroids = new_centroids[:]\n",
    "        \n",
    "        # do one more step\n",
    "        new_centroids = one_step(points, centroids, clusters)\n",
    "        \n",
    "    return new_centroids, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, print each cluster on a 2D plan as well as all the points contained inside them. Assign a different color for each group. Do you see something that seems correct ? (i.e. as a human, what would have you done ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_centroids, clusters = kmeans(points, K)\n",
    "plt.scatter(points[:, 0], points[:, 1], c=clusters, s=2)\n",
    "plt.scatter(final_centroids[:,0], final_centroids[:,1],\n",
    "            marker=\"x\", c=\"red\")\n",
    "plt.show()\n",
    "\n",
    "# the clustering is correct, and each centroid found by the\n",
    "# algorithm is in the middle of its cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run again your algorithm but with a different value for k. What happens ? Do you think it is a good idea to put a high value for k (k > 10) when the number of clusters is small (< 5) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_centroids, clusters = kmeans(points, 12)\n",
    "plt.scatter(points[:, 0], points[:, 1], c=clusters, s=2)\n",
    "plt.scatter(final_centroids[:,0], final_centroids[:,1],\n",
    "            marker=\"x\", c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : when K is high compared to the real number of clusters, the algorithm separate clusters into subclusters. This is not correct because 2 points that were close in the original dataset (and belong to the same cluster) will be into 2 different clusters. The optimal value for K should match the number of real cluster.\n",
    "\n",
    "If it is not possible to guess the real number of clusters of the original dataset, one can use some metrics (such as the inter-cluster distance or the intra-cluster distance) to find the optimal value for K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized version of k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a library that is **very optimized for vectorized operations** : instead of doing operations element by element on an array, the operation is done on the whole array, to speed up computation. In our case, we can optimize the k-means algorithm by computing the distance between each point and all the centroids with only 1 operation (instead of iterating all the points, then all the centroids, then choosing the one with minimal distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_optimized(points, K):\n",
    "    # initialization\n",
    "    centroids = points[np.random.randint(0, len(points), K)]\n",
    "    \n",
    "    # one first step\n",
    "    distances = np.apply_along_axis(distance, 1, points, \n",
    "                                    v2=centroids, ax=1)\n",
    "    clusters = distances.argmin(axis=1)\n",
    "    new_centroids = np.zeros_like(centroids)\n",
    "    for k in range(K):\n",
    "        idx = np.argwhere(clusters == k)\n",
    "        new_centroids[k] = points[idx].sum(axis=0) / len(idx)\n",
    "\n",
    "    e = 0.1\n",
    "    while distance(new_centroids, centroids, 1).sum() > e:\n",
    "        # copy the new_centroids into centroids\n",
    "        centroids = new_centroids[:]\n",
    "        \n",
    "        # do one more step\n",
    "        clusters = np.apply_along_axis(distance, 1, points, \n",
    "                     v2=centroids, ax=1).argmin(axis=1)\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for k in range(K):\n",
    "            idx = np.argwhere(clusters == k)\n",
    "            new_centroids[k] = points[idx].sum(axis=0)\n",
    "            new_centroids[k] /= len(idx)\n",
    "    \n",
    "    return new_centroids, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the kmeans_optimized function\n",
    "final_centroids, clusters = kmeans_optimized(points, 3)\n",
    "plt.scatter(points[:, 0], points[:, 1], c=clusters, s=2)\n",
    "plt.scatter(final_centroids[:,0], final_centroids[:,1],\n",
    "            marker=\"x\", c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the performance improvement brought by the optimized version of the k-means algorithms with the magic command `%time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = kmeans(points, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = kmeans_optimized(points, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = kmeans(points, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = kmeans_optimized(points, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have almost a x10 speed improvement. The improvement is more visible when K is high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
